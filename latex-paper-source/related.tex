% !TEX root = template.tex

\section{Related Work}
\label{sec:related_work}

In the past, HAR approaches were developed by following the standard pipeline in pattern recognition activities: segment extraction from an input time-series sequence, computing human engineered features and predicting class labels. Various modeling algorithms such as Support Vector Machine (SVM), Naive Bayes \cite{base-paper}, Random Forest or sequence based approaches such as Dynamic Time Warping \cite{Daniel-2015} or Hidden Markov Model \cite{Andreas-2014, Francisco-2014} were frequently used.

However, the performance of the aforementioned approaches is heavily dependant on the hand-crafted features. Extracting such features for machine learning systems is subjective, more error prone and can result in poor expressivity of the feature set.
In \cite{base-paper} the authors manually chose 19 features, derived from various sensor measures, which were carefully selected because of their discriminative power between the various activities that were tracked. Their work applies and compares different Bayesian estimation techniques. In most cases, the last activity a person has performed influences their current activity, this knowledge can provide valuable input for the network. The work on this paper represents the baseline which I wanted to improve upon, by using more recent techniques and automatic feature extraction.\\
In \cite{Grzezick-2017} the authors use a CNN for human activity recognition activities for collecting and analyzing realistic data from manual processes in industrial scenarios. The experiment was carried out using multiple IMU sensors and the data collected was augmented using gaussian noise and random resampling. Data was then segmented and fed to a CNN using only temporal convolutions. The CNN performed consistently better compared to other techniques such as Bayes, Random Forest and SVM.\\
Approaches that are able to exploit the temporal dependencies in time-series data appear as the natural choice for modelling human movement captured with sensor data. Deep recurrent neural networks (RNNs), especially those that rely on Long Short-Term Memory cells (LSTMs), have recently achieved impressive performance across a variety of scenarios. Many recent papers explore this approach and compare with a baseline CNN. Each LSTM unit keeps track of an internal state that represents it's "memory". Over time the cells learn to output, overwrite, or reset their internal memory based on their current input and the history of past internal states, leading to a system capable of retaining information across hundreds of time-steps. In \cite{nils-2016} the authors implement two flavours of LSTM recurrent networks: a deep forward LSTMs containing multiple layers of recurrent units that are connected "forward" in time and bi-directional LSTMs which contain two parallel recurrent layers that stretch both into the "future" and into the "past". Testing against 3 well known benchmark datasets in the HAR field, the recurrent nn achieves higher classification accuracy in all but one dataset.\\
\cite{Valarezo-2017} combines effective preprocessing techniques i.e. downsampling of the input raw data and RNNs to prove how this technique achieves 9\% better accuracy compared to a 1-D CNN on a benchmark dataset.\\
Extending on the process of HAR, some researchers were successful in performing activity recognition tasks using depth cameras \cite{su-2016, alessandro-2016} which are nowadays widely available and affordable.\\
\cite{2017-mfi-actionrecognition} evolved on this concept by merging input data coming from a single rgb camera and sensor values coming from a wrist-worn IMU sensor. Their proposed feature extractor for the IMU sensor is based on a convolutional autoencoder while for feature extraction in the rgb image a CNN is used, leveraging residual modules for a better propagation of the gradients during training. The usage of both camera signal and IMU signals improved the classification accuracy and prevented performance degeneration caused by the failure of joint estimation due to eg. image occlusion and noise.